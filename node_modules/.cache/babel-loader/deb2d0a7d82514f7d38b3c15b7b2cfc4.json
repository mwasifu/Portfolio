{"ast":null,"code":"var _jsxFileName = \"/Users/wasif/Documents/GitHub/portfolio/src/components/Face.jsx\";\nimport React from 'react';\nimport { blazeface } from '@tensorflow-models/blazeface';\n;\n\nconst Face = () => {\n  const model = blazeface.load(); // Pass in an image or video to the model. The model returns an array of\n  // bounding boxes, probabilities, and landmarks, one for each detected face.\n\n  if ('mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices) {\n    console.log(\"Let's get this party started\");\n  }\n\n  navigator.mediaDevices.getUserMedia({\n    video: true\n  });\n  const returnTensors = false; // Pass in `true` to get tensors back, rather than values.\n\n  const predictions = model.estimateFaces(document.querySelector(\"img\"), returnTensors);\n\n  if (predictions.length > 0) {\n    [{\n      topLeft: [232.28, 145.26],\n      bottomRight: [449.75, 308.36],\n      probability: [0.998],\n      landmarks: [[295.13, 177.64], // right eye\n      [382.32, 175.56], // left eye\n      [341.18, 205.03], // nose\n      [345.12, 250.61], // mouth\n      [252.76, 211.37], // right ear\n      [431.20, 204.93] // left ear\n      ]\n    }];\n\n    for (let i = 0; i < predictions.length; i++) {\n      const start = predictions[i].topLeft;\n      const end = predictions[i].bottomRight;\n      const size = [end[0] - start[0], end[1] - start[1]]; // Render a rectangle over each detected face.\n\n      ctx.fillRect(start[0], start[1], size[0], size[1]);\n    }\n  }\n\n  return /*#__PURE__*/React.createElement(\"div\", {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 45,\n      columnNumber: 9\n    }\n  });\n};\n\nexport default Face;","map":{"version":3,"sources":["/Users/wasif/Documents/GitHub/portfolio/src/components/Face.jsx"],"names":["React","blazeface","Face","model","load","navigator","mediaDevices","console","log","getUserMedia","video","returnTensors","predictions","estimateFaces","document","querySelector","length","topLeft","bottomRight","probability","landmarks","i","start","end","size","ctx","fillRect"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAASC,SAAT,QAAyB,8BAAzB;AAAwD;;AAExD,MAAMC,IAAI,GAAG,MAAM;AACf,QAAMC,KAAK,GAAIF,SAAS,CAACG,IAAV,EAAf,CADe,CAGjB;AACA;;AACA,MAAI,kBAAkBC,SAAlB,IAA+B,kBAAkBA,SAAS,CAACC,YAA/D,EAA6E;AAC3EC,IAAAA,OAAO,CAACC,GAAR,CAAY,8BAAZ;AACD;;AACDH,EAAAA,SAAS,CAACC,YAAV,CAAuBG,YAAvB,CAAoC;AAACC,IAAAA,KAAK,EAAE;AAAR,GAApC;AAEA,QAAMC,aAAa,GAAG,KAAtB,CAViB,CAUY;;AAC7B,QAAMC,WAAW,GAAIT,KAAK,CAACU,aAAN,CAAoBC,QAAQ,CAACC,aAAT,CAAuB,KAAvB,CAApB,EAAmDJ,aAAnD,CAArB;;AAEA,MAAIC,WAAW,CAACI,MAAZ,GAAqB,CAAzB,EAA4B;AAC1B,KACE;AACEC,MAAAA,OAAO,EAAE,CAAC,MAAD,EAAS,MAAT,CADX;AAEEC,MAAAA,WAAW,EAAE,CAAC,MAAD,EAAS,MAAT,CAFf;AAGEC,MAAAA,WAAW,EAAE,CAAC,KAAD,CAHf;AAIEC,MAAAA,SAAS,EAAE,CACT,CAAC,MAAD,EAAS,MAAT,CADS,EACS;AAClB,OAAC,MAAD,EAAS,MAAT,CAFS,EAES;AAClB,OAAC,MAAD,EAAS,MAAT,CAHS,EAGS;AAClB,OAAC,MAAD,EAAS,MAAT,CAJS,EAIS;AAClB,OAAC,MAAD,EAAS,MAAT,CALS,EAKS;AAClB,OAAC,MAAD,EAAS,MAAT,CANS,CAMQ;AANR;AAJb,KADF;;AAiBA,SAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGT,WAAW,CAACI,MAAhC,EAAwCK,CAAC,EAAzC,EAA6C;AAC3C,YAAMC,KAAK,GAAGV,WAAW,CAACS,CAAD,CAAX,CAAeJ,OAA7B;AACA,YAAMM,GAAG,GAAGX,WAAW,CAACS,CAAD,CAAX,CAAeH,WAA3B;AACA,YAAMM,IAAI,GAAG,CAACD,GAAG,CAAC,CAAD,CAAH,GAASD,KAAK,CAAC,CAAD,CAAf,EAAoBC,GAAG,CAAC,CAAD,CAAH,GAASD,KAAK,CAAC,CAAD,CAAlC,CAAb,CAH2C,CAK3C;;AACAG,MAAAA,GAAG,CAACC,QAAJ,CAAaJ,KAAK,CAAC,CAAD,CAAlB,EAAuBA,KAAK,CAAC,CAAD,CAA5B,EAAiCE,IAAI,CAAC,CAAD,CAArC,EAA0CA,IAAI,CAAC,CAAD,CAA9C;AACD;AACF;;AACC,sBACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IADJ;AAKH,CA7CD;;AA+CA,eAAetB,IAAf","sourcesContent":["import React from 'react'\nimport  {blazeface} from '@tensorflow-models/blazeface';;\n\nconst Face = () => {\n    const model =  blazeface.load();\n\n  // Pass in an image or video to the model. The model returns an array of\n  // bounding boxes, probabilities, and landmarks, one for each detected face.\n  if ('mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices) {\n    console.log(\"Let's get this party started\")\n  }\n  navigator.mediaDevices.getUserMedia({video: true})\n\n  const returnTensors = false; // Pass in `true` to get tensors back, rather than values.\n  const predictions =  model.estimateFaces(document.querySelector(\"img\"), returnTensors);\n\n  if (predictions.length > 0) {\n    [\n      {\n        topLeft: [232.28, 145.26],\n        bottomRight: [449.75, 308.36],\n        probability: [0.998],\n        landmarks: [\n          [295.13, 177.64], // right eye\n          [382.32, 175.56], // left eye\n          [341.18, 205.03], // nose\n          [345.12, 250.61], // mouth\n          [252.76, 211.37], // right ear\n          [431.20, 204.93] // left ear\n        ]\n      }\n    ]\n  \n\n    for (let i = 0; i < predictions.length; i++) {\n      const start = predictions[i].topLeft;\n      const end = predictions[i].bottomRight;\n      const size = [end[0] - start[0], end[1] - start[1]];\n\n      // Render a rectangle over each detected face.\n      ctx.fillRect(start[0], start[1], size[0], size[1]);\n    }\n  }\n    return (\n        <div>\n            \n        </div>\n    )\n}\n\nexport default Face\n"]},"metadata":{},"sourceType":"module"}